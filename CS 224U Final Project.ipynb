{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224U Final Project: Relation Extraction with Graph Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** Ben Barnett and Aakash Pattabi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "import sys\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading TACRED data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': '61b3a65fb906688c92a1', 'relation': 'no_relation', 'token': ['Ali', 'lied', 'about', 'having', 'to', 'leave', 'for', 'her', 'job', 'to', 'see', 'if', 'Jake', 'would', 'end', 'the', 'show', 'to', 'be', 'with', 'her', '.'], 'subj_start': 20, 'subj_end': 20, 'obj_start': 12, 'obj_end': 12, 'subj_type': 'PERSON', 'obj_type': 'PERSON', 'stanford_pos': ['NNP', 'VBD', 'IN', 'VBG', 'TO', 'VB', 'IN', 'PRP$', 'NN', 'TO', 'VB', 'IN', 'NNP', 'MD', 'VB', 'DT', 'NN', 'TO', 'VB', 'IN', 'PRP', '.'], 'stanford_ner': ['PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'PERSON', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'], 'stanford_head': ['2', '0', '4', '2', '6', '4', '11', '9', '11', '11', '6', '15', '15', '15', '11', '17', '15', '21', '21', '21', '17', '2'], 'stanford_deprel': ['nsubj', 'ROOT', 'mark', 'advcl', 'mark', 'xcomp', 'mark', 'nmod:poss', 'nsubj', 'mark', 'advcl', 'mark', 'nsubj', 'aux', 'advcl', 'det', 'dobj', 'mark', 'cop', 'case', 'acl', 'punct']}\n"
     ]
    }
   ],
   "source": [
    "train_path = \"tacred-relation/dataset/tacred/train.json\"\n",
    "eval_path = \"tacred-relation/dataset/tacred/dev.json\"\n",
    "test_path = \"tacred-relation/dataset/tacred/test.json\"\n",
    "\n",
    "with open(train_path, \"rb\") as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(eval_path, \"rb\") as f:\n",
    "    eval_data = json.load(f)\n",
    "    \n",
    "with open(test_path, \"rb\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "sanity = train_data[0]\n",
    "print(sanity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining a sentence-level feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphFeatureExtractor(object):\n",
    "    def __init__(self, server, parse_level = \"basic\", \n",
    "                 embedding_path = None, \n",
    "                 save_path = None):\n",
    "        self.server = server\n",
    "        self.set_parse_level(parse_level)\n",
    "        \n",
    "        # Initialize word embeddings\n",
    "        if embedding_path:\n",
    "            self.embedding_path = embedding_path\n",
    "            self._load_embeddings()\n",
    "        else:\n",
    "            self.embedding_path = None\n",
    "    \n",
    "    def _load_embeddings(self):\n",
    "        self.embeddings = {}\n",
    "        with open(self.embedding_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                tokens = line.split()\n",
    "                self.embeddings[tokens[0]] = [float(i) for i in tokens[1:]]\n",
    "            \n",
    "    def set_parse_level(self, parse_level):\n",
    "        assert parse_level in [\"basic\", \"enhanced\", \"extra_enhanced\"]\n",
    "        d = {\n",
    "            \"basic\" : \"basicDependencies\",\n",
    "            \"enhanced\" : \"enhancedDependencies\", \n",
    "            \"extra_enhanced\" : \"enhancedPlusPlusDependencies\"\n",
    "        }\n",
    "        self.parse_level = d[parse_level]\n",
    "    \n",
    "    def extract_graph(self, sentence):\n",
    "        Y = sentence[\"relation\"]\n",
    "        \n",
    "        # Extract tokens, subsentence (b/w subj->obj tokens)\n",
    "        tokens = sentence[\"token\"]\n",
    "        first = min(sentence[\"subj_end\"], sentence[\"obj_end\"])\n",
    "        second = max(sentence[\"subj_start\"], sentence[\"obj_start\"])\n",
    "        middle = tokens[first+1:second]\n",
    "\n",
    "        # Concatenate full sentence and sentence middle (b/w subj->obj tokens)\n",
    "        full_sentence = \" \".join(tokens)\n",
    "        full_middle = \" \".join(middle)\n",
    "        \n",
    "        # Parse with Stanford parser\n",
    "        full_sentence_out = server.annotate(full_sentence, properties = {\n",
    "            \"annotators\" : \"parse\", \n",
    "            \"outputFormat\" : \"json\"\n",
    "        })\n",
    "        middle_out = server.annotate(full_middle, properties = {\n",
    "            \"annotators\" : \"parse\", \n",
    "            \"outputFormat\" : \"json\"\n",
    "        })\n",
    "        \n",
    "        # Extract graph edgelist\n",
    "        X_full = self._parse_to_graph(full_sentence_out)\n",
    "        X_middle = self._parse_to_graph(middle_out)\n",
    "        \n",
    "        # Add word-level GloVe features to graph inputs\n",
    "        if self.embedding_path:\n",
    "            X_full[\"features\"] = self._get_embedding_features(tokens)\n",
    "            X_middle[\"features\"] = self._get_embedding_features(middle)\n",
    "        \n",
    "        return {\"full\" : X_full, \"middle\" : X_middle, \"Y\" : Y}\n",
    "        \n",
    "    def _parse_to_graph(self, parse):\n",
    "        dep_list = parse[\"sentences\"][0][self.parse_level]\n",
    "        dep_graph = defaultdict(lambda : [])\n",
    "        for d in dep_list:\n",
    "            dep_graph[d[\"governor\"]].append(d[\"dependent\"])\n",
    "        return self._convert_to_edgelist(dep_graph)\n",
    "    \n",
    "    def _convert_to_edgelist(self, dep_graph):\n",
    "        el = {\n",
    "            \"edges\" : [[k, vi] for k, v in dep_graph.items() for vi in v] \n",
    "        }        \n",
    "        return el\n",
    "    \n",
    "    def _get_embedding_features(self, sent, embedding_dim = 50):\n",
    "        feats = {}\n",
    "        for i, token in enumerate(sent):\n",
    "            features = self.embeddings.get(token, None)\n",
    "            if not features:\n",
    "                features = [np.random.rand() for j in range(embedding_dim)]\n",
    "            feats[i+1] = features\n",
    "            \n",
    "        # By default, we assign the [ROOT] token in the parse tree an embedding vector\n",
    "        # of all zeroes... mostly because I'm not strictly sure what else to do here. @Ben, thoughts?\n",
    "        feats[0] = [0]*embedding_dim\n",
    "        return feats\n",
    "    \n",
    "    def extract_batch_graphs(self, sentences):\n",
    "        return [self.extract_graph(s) for s in sentences]\n",
    "    \n",
    "    def save_jsons(self, graphs, save_path, postfix = \"\"):\n",
    "        assert save_path and save_path[-1] == \"/\"\n",
    "        for i, g in enumerate(graphs):\n",
    "            with open(save_path + str(i) + postfix + \".json\", \"w\") as f:\n",
    "                json.dump(g, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we generate all graph features (with 50-dimensional GloVe) vectors for the TACRED training set. Each sentence-level graph (over the entire sentence and over only the \"bridge\" words between the subject and the object) is saved to a .json file which we then post-process with Graph2Vec. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = \"./glove/glove.6B.50d.txt\"\n",
    "server = StanfordCoreNLP(\"http://localhost:9000\")\n",
    "\n",
    "fe = GraphFeatureExtractor(server, embedding_path = embedding_path)\n",
    "feats = fe.extract_batch_graphs(train_data)\n",
    "\n",
    "full_save_path = \"./train_features_full/\"\n",
    "fe.save_jsons([feat[\"full\"] for feat in feats], postfix = \"\", save_path = full_save_path)\n",
    "\n",
    "middle_save_path = \"./train_features_middle/\"\n",
    "fe.save_jsons([feat[\"middle\"] for feat in feats], postfix = \"\", save_path = middle_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24])\n",
      "[[0, 3], [3, 1], [3, 2], [3, 10], [3, 25], [10, 4], [10, 13], [4, 5], [4, 6], [4, 7], [4, 8], [4, 9], [13, 11], [13, 12], [13, 17], [17, 14], [17, 16], [17, 19], [17, 24], [16, 15], [19, 18], [24, 20], [24, 21], [24, 22], [24, 23]]\n",
      "['So', 'what', 'do', 'Fisher', ',', 'Kiffin', ',', 'and', 'Marciano', 'see', 'unfolding', 'Saturday', 'night', 'when', 'the', 'Jaguars', 'visit', 'the', 'Patriots', 'in', 'an', 'AFC', 'playoff', 'matchup', '?']\n",
      "[{'dep': 'ROOT', 'governor': 0, 'governorGloss': 'ROOT', 'dependent': 2, 'dependentGloss': 'lied'}, {'dep': 'nsubj', 'governor': 2, 'governorGloss': 'lied', 'dependent': 1, 'dependentGloss': 'Ali'}, {'dep': 'mark', 'governor': 4, 'governorGloss': 'having', 'dependent': 3, 'dependentGloss': 'about'}, {'dep': 'advcl', 'governor': 2, 'governorGloss': 'lied', 'dependent': 4, 'dependentGloss': 'having'}, {'dep': 'mark', 'governor': 6, 'governorGloss': 'leave', 'dependent': 5, 'dependentGloss': 'to'}, {'dep': 'xcomp', 'governor': 4, 'governorGloss': 'having', 'dependent': 6, 'dependentGloss': 'leave'}, {'dep': 'case', 'governor': 9, 'governorGloss': 'job', 'dependent': 7, 'dependentGloss': 'for'}, {'dep': 'nmod:poss', 'governor': 9, 'governorGloss': 'job', 'dependent': 8, 'dependentGloss': 'her'}, {'dep': 'nmod', 'governor': 6, 'governorGloss': 'leave', 'dependent': 9, 'dependentGloss': 'job'}, {'dep': 'mark', 'governor': 11, 'governorGloss': 'see', 'dependent': 10, 'dependentGloss': 'to'}, {'dep': 'acl', 'governor': 9, 'governorGloss': 'job', 'dependent': 11, 'dependentGloss': 'see'}, {'dep': 'mark', 'governor': 15, 'governorGloss': 'end', 'dependent': 12, 'dependentGloss': 'if'}, {'dep': 'nsubj', 'governor': 15, 'governorGloss': 'end', 'dependent': 13, 'dependentGloss': 'Jake'}, {'dep': 'aux', 'governor': 15, 'governorGloss': 'end', 'dependent': 14, 'dependentGloss': 'would'}, {'dep': 'advcl', 'governor': 11, 'governorGloss': 'see', 'dependent': 15, 'dependentGloss': 'end'}, {'dep': 'det', 'governor': 17, 'governorGloss': 'show', 'dependent': 16, 'dependentGloss': 'the'}, {'dep': 'dobj', 'governor': 15, 'governorGloss': 'end', 'dependent': 17, 'dependentGloss': 'show'}, {'dep': 'mark', 'governor': 21, 'governorGloss': 'her', 'dependent': 18, 'dependentGloss': 'to'}, {'dep': 'cop', 'governor': 21, 'governorGloss': 'her', 'dependent': 19, 'dependentGloss': 'be'}, {'dep': 'case', 'governor': 21, 'governorGloss': 'her', 'dependent': 20, 'dependentGloss': 'with'}, {'dep': 'acl', 'governor': 17, 'governorGloss': 'show', 'dependent': 21, 'dependentGloss': 'her'}, {'dep': 'punct', 'governor': 2, 'governorGloss': 'lied', 'dependent': 22, 'dependentGloss': '.'}]\n"
     ]
    }
   ],
   "source": [
    "X_full = [feat[\"full\"] for feat in feats]\n",
    "embs = X_full[7][\"features\"]\n",
    "print(embs.keys())\n",
    "print(X_full[7][\"edges\"])\n",
    "print(train_data[7][\"token\"])\n",
    "\n",
    "dp = server.annotate(\" \".join(train_data[0][\"token\"]), properties = {\n",
    "            \"annotators\" : \"parse\", \n",
    "            \"outputFormat\" : \"json\"\n",
    "        })\n",
    "dpd = dp[\"sentences\"][0][\"basicDependencies\"]\n",
    "print(dpd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining a test harness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Evaluating the model and comparing to baselines"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
