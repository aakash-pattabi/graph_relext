{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 224U Final Project: Relation Extraction with Graph Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Authors:** Ben Barnett and Aakash Pattabi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import sys\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from collections import defaultdict\n",
    "import subprocess\n",
    "import sys\n",
    "import random\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading TACRED data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_path = \"tacred/data/json/train.json\"\n",
    "eval_path = \"tacred/data/json/dev.json\"\n",
    "test_path = \"tacred/data/json/test.json\"\n",
    "\n",
    "with open(train_path, \"rb\") as f:\n",
    "    train_data = json.load(f)\n",
    "    \n",
    "with open(eval_path, \"rb\") as f:\n",
    "    eval_data = json.load(f)\n",
    "    \n",
    "with open(test_path, \"rb\") as f:\n",
    "    test_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"{} training samples\".format(len(train_data)))\n",
    "print(\"{} dev samples\".format(len(eval_data)))\n",
    "print(\"{} test samples\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Defining a sentence-level feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GraphFeatureExtractor(object):\n",
    "    def __init__(self, server, parse_level = \"basic\", \n",
    "                 embedding_path = None):\n",
    "        self.server = server\n",
    "        self.set_parse_level(parse_level)\n",
    "        \n",
    "        # Initialize word embeddings\n",
    "        if embedding_path:\n",
    "            self.embedding_path = embedding_path\n",
    "            self._load_embeddings()\n",
    "        else:\n",
    "            self.embedding_path = None\n",
    "            \n",
    "        self._reset_class_distribution()\n",
    "        self._reset_class_labels()\n",
    "    \n",
    "    def _reset_class_labels(self):\n",
    "        self.class_labels = {}\n",
    "    \n",
    "    def _reset_class_distribution(self):\n",
    "        self.class_distribution = defaultdict(int)\n",
    "        self.sentences_seen = 0\n",
    "        \n",
    "    def _load_embeddings(self):\n",
    "        self.embeddings = {}\n",
    "        with open(self.embedding_path, \"r\", encoding = \"utf-8\") as f:\n",
    "            for line in f:\n",
    "                tokens = line.split()\n",
    "                self.embeddings[tokens[0]] = [float(i) for i in tokens[1:]]\n",
    "    \n",
    "    def get_class_labels(self):\n",
    "        return self.class_labels\n",
    "    \n",
    "    def set_parse_level(self, parse_level):\n",
    "        assert parse_level in [\"basic\", \"enhanced\", \"extra_enhanced\"]\n",
    "        d = {\n",
    "            \"basic\" : \"basicDependencies\",\n",
    "            \"enhanced\" : \"enhancedDependencies\", \n",
    "            \"extra_enhanced\" : \"enhancedPlusPlusDependencies\"\n",
    "        }\n",
    "        self.parse_level = d[parse_level]\n",
    "    \n",
    "    def _extract_graph(self, sentence):\n",
    "        Y = sentence[\"relation\"]\n",
    "        self.class_distribution[Y] += 1\n",
    "        self.sentences_seen += 1\n",
    "        if Y not in self.class_labels:\n",
    "            self.class_labels[Y] = len(self.class_labels)\n",
    "        \n",
    "        # Extract tokens, subsentence (b/w subj->obj tokens)\n",
    "        tokens = sentence[\"token\"]\n",
    "        first = min(sentence[\"subj_start\"], sentence[\"obj_start\"])\n",
    "        second = max(sentence[\"subj_end\"], sentence[\"obj_end\"])\n",
    "        middle = tokens[first:second+1]\n",
    "\n",
    "        # Concatenate full sentence and sentence middle (b/w subj->obj tokens)\n",
    "        full_sentence = \" \".join(tokens)\n",
    "        full_middle = \" \".join(middle)\n",
    "        \n",
    "        # Parse with Stanford parser\n",
    "        full_sentence_out = server.annotate(full_sentence, properties = {\n",
    "            \"annotators\" : \"parse\", \n",
    "            \"outputFormat\" : \"json\"\n",
    "        })\n",
    "        middle_out = server.annotate(full_middle, properties = {\n",
    "            \"annotators\" : \"parse\", \n",
    "            \"outputFormat\" : \"json\"\n",
    "        })\n",
    "        \n",
    "        # Extract graph edgelist\n",
    "        X_full, full_tokens = self._parse_to_graph(full_sentence_out)\n",
    "        X_middle, middle_tokens = self._parse_to_graph(middle_out)\n",
    "        \n",
    "        # Add word-level GloVe features to graph inputs\n",
    "        if self.embedding_path:\n",
    "            X_full[\"features\"] = self._get_embedding_features(full_tokens)\n",
    "            for k in set().union(*X_full[\"edges\"]): # Error-checking to make sure G2V completes\n",
    "                assert k in X_full[\"features\"]\n",
    "            \n",
    "            X_middle[\"features\"] = self._get_embedding_features(middle_tokens)\n",
    "            for k in set().union(*X_middle[\"edges\"]): # Error-checking to make sure G2V completes\n",
    "                assert k in X_middle[\"features\"]\n",
    "        \n",
    "        return {\"full\" : X_full, \"middle\" : X_middle, \"Y\" : Y}\n",
    "        \n",
    "    def _parse_to_graph(self, parse):                        \n",
    "        dep_list = parse[\"sentences\"][0][self.parse_level]\n",
    "        dep_graph = defaultdict(lambda : [])\n",
    "        for d in dep_list:\n",
    "                dep_graph[d[\"governor\"]].append(d[\"dependent\"])\n",
    "        \n",
    "        parser_tokens = [tok[\"word\"] for tok in parse[\"sentences\"][0][\"tokens\"]]\n",
    "        return self._convert_to_edgelist(dep_graph), parser_tokens\n",
    "    \n",
    "    def _convert_to_edgelist(self, dep_graph):\n",
    "        el = {\n",
    "            \"edges\" : [[k, vi] for k, v in dep_graph.items() for vi in v] \n",
    "        }        \n",
    "        return el\n",
    "    \n",
    "    def _get_embedding_features(self, sent, embedding_dim = 50):\n",
    "        feats = {}\n",
    "        for i, token in enumerate(sent):\n",
    "            features = self.embeddings.get(token, None)\n",
    "            if not features:\n",
    "                features = [np.random.rand() for j in range(embedding_dim)]\n",
    "            feats[i+1] = features\n",
    "            \n",
    "        # By default, we assign the [ROOT] token in the parse tree an embedding vector\n",
    "        # of all zeroes... mostly because I'm not strictly sure what else to do here. @Ben, thoughts?\n",
    "        feats[0] = [0]*embedding_dim\n",
    "        return feats\n",
    "    \n",
    "    def extract_batch_graphs(self, sentences):\n",
    "        n_sentences = len(sentences)\n",
    "        batch = []\n",
    "        for i, s in enumerate(sentences):\n",
    "            batch.append(self._extract_graph(s))\n",
    "            print(\"Extracted graphs for [{}/{}] sentences...\".format(i+1, n_sentences), end = \"\\r\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        return batch\n",
    "    \n",
    "    # We ensure that none of the \"no_relation\" relations are resampled\n",
    "    def extract_resampled_graphs(self, features, total):\n",
    "        n_to_resample = total - len(features)\n",
    "        if n_to_resample <= 0:\n",
    "            return\n",
    "        \n",
    "        resampled_features = []\n",
    "        while n_to_resample > 0:\n",
    "            feat = random.choice(features)\n",
    "            rel = feat[\"Y\"]\n",
    "            if rel != \"no_relation\":\n",
    "                frequency = self.class_distribution[rel]/self.sentences_seen\n",
    "                keep = (np.random.rand() > frequency)\n",
    "                if keep:\n",
    "                    resampled_features.append(feat)\n",
    "                    n_to_resample -= 1\n",
    "        return resampled_features\n",
    "    \n",
    "    def save_jsons(self, graphs, save_path):\n",
    "        assert save_path and save_path[-1] == \"/\"\n",
    "        for i, g in enumerate(graphs):\n",
    "            with open(save_path + str(i) + \".json\", \"w\") as f:\n",
    "                json.dump(g, f)\n",
    "    \n",
    "    def concat_with_mean_embedding(self, features, embedding_dicts):\n",
    "        n, d = features.shape\n",
    "        mean_embeddings = []\n",
    "        for i in range(n):\n",
    "            mean_embeddings.append(np.mean([v for k, v in embedding_dicts[i].items()], axis = 0))\n",
    "        return np.concatenate((features, mean_embeddings), axis = 1)\n",
    "    \n",
    "    def concat_with_sum_embedding(self, features, embedding_dicts):\n",
    "        n, d = features.shape\n",
    "        sum_embeddings = []\n",
    "        for i in range(n):\n",
    "            sum_embeddings.append(np.sum([v for k, v in embedding_dicts[i].items()], axis = 0))\n",
    "        return np.concatenate((features, sum_embeddings), axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional features to implement:\n",
    "1. ~~Resampling of the original data to augment data size + reduce class imbalance that *doesn't* duplicate Graph2Vec features across repeated samples.~~\n",
    "2. ~~Creating a full TAC dataset-to-features pipeline that can be deployed on arbitrary data~~ \n",
    "3. Incorporation of e.g. parse tree label information into the features. \n",
    "4. ~~Classifiers - implement logistic regression, set up as a neural network.~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Defining a full feature-extraction pipeline that can be used on arbitrary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class FeatureExtractorPipeline(object):\n",
    "    def __init__(self, fe_args, save_paths):\n",
    "        self.fe = GraphFeatureExtractor(**fe_args)\n",
    "        self.save_paths = save_paths\n",
    "        self.class_labels = None\n",
    "    \n",
    "    def _one_hot(self, length, hot):\n",
    "        a = np.zeros(length)\n",
    "        a[hot] = 1\n",
    "        return a\n",
    "    \n",
    "    def _extract_training_graphs(self, sentences, total, train_flag):\n",
    "        feats = self.fe.extract_batch_graphs(sentences)\n",
    "        if total > len(sentences):\n",
    "            resampled_feats = self.fe.extract_resampled_graphs(feats, total)\n",
    "            feats += resampled_feats\n",
    "            assert len(feats) == total    \n",
    "        \n",
    "        self.fe.save_jsons(\n",
    "            [feat[\"full\"] for feat in feats], \n",
    "            self.save_paths[train_flag + \"_full_save_path\"]\n",
    "        )\n",
    "        self.fe.save_jsons(\n",
    "            [feat[\"middle\"] for feat in feats], \n",
    "            self.save_paths[train_flag + \"_middle_save_path\"]\n",
    "        )\n",
    "        \n",
    "        self.class_labels = self.fe.get_class_labels()\n",
    "        n_classes = len(self.class_labels)\n",
    "        Y = [self._one_hot(n_classes, self.class_labels[feat[\"Y\"]]) for feat in feats]\n",
    "        X_full_embs = [feat[\"full\"][\"features\"] for feat in feats]\n",
    "        X_middle_embs = [feat[\"middle\"][\"features\"] for feat in feats]\n",
    "        \n",
    "        return Y, X_full_embs, X_middle_embs\n",
    "    \n",
    "    def _extract_training_embeddings(self, \n",
    "                                     X_full_embs, X_middle_embs,\n",
    "                                     train_flag, \n",
    "                                     concat = \"mean\"):\n",
    "        assert concat in [\"mean\", \"sum\", \"both\"]\n",
    "        \n",
    "        success_full = subprocess.call(\n",
    "            [\"python\", \n",
    "             \"graph2vec/src/graph2vec.py\", \n",
    "             \"--input-path\", self.save_paths[train_flag + \"_full_save_path\"], \n",
    "             \"--output-path\", self.save_paths[train_flag + \"_full_g2v_path\"]]\n",
    "        )\n",
    "        assert success_full == 0\n",
    "        \n",
    "        success_middle = subprocess.call(\n",
    "            [\"python\", \n",
    "             \"graph2vec/src/graph2vec.py\", \n",
    "             \"--input-path\", self.save_paths[train_flag + \"_middle_save_path\"], \n",
    "             \"--output-path\", self.save_paths[train_flag + \"_middle_g2v_path\"]]\n",
    "        )\n",
    "        assert success_middle == 0\n",
    "        \n",
    "        X_full = pd.read_csv(\n",
    "            self.save_paths[train_flag + \"_full_g2v_path\"], sep = \",\").drop(labels = [\"type\"], axis = 1)\n",
    "        X_full = np.array(X_full)\n",
    "        \n",
    "        X_middle = pd.read_csv(\n",
    "            self.save_paths[train_flag + \"_middle_g2v_path\"], sep = \",\").drop(labels = [\"type\"], axis = 1)\n",
    "        X_middle = np.array(X_middle)\n",
    "        \n",
    "        if concat == \"mean\" or concat == \"both\":\n",
    "            X_full = self.fe.concat_with_mean_embedding(X_full, X_full_embs)\n",
    "            X_middle = self.fe.concat_with_mean_embedding(X_middle, X_middle_embs)\n",
    "        elif concat == \"sum\" or concat == \"both\":\n",
    "            X_full = self.fe.concat_with_sum_embedding(X_full, X_full_embs)\n",
    "            X_middle = self.fe.concat_with_sum_embedding(X_middle, X_middle_embs)\n",
    "        \n",
    "        return X_full, X_middle\n",
    "    \n",
    "    def get_class_labels(self):\n",
    "        return self.class_labels\n",
    "    \n",
    "    def extract_features(self, sentences, total = 0, \n",
    "                         train_flag = \"train\", concat = \"mean\"):\n",
    "        assert train_flag in [\"train\", \"test\", \"eval\"]\n",
    "        \n",
    "        print(\"Building graphs from sentence dependency trees...\")\n",
    "        Y, X_full_embs, X_middle_embs = self._extract_training_graphs(sentences, total, train_flag)\n",
    "        np.save(self.save_paths[\"responses\"] + train_flag, Y)\n",
    "        print(\"Resampled sentence graphs up to {} total examples in the data...\".format(len(Y)), end = \"\\r\")\n",
    "            \n",
    "        print(\"Extracting Graph2Vec embeddings and annotating the examples w/ GloVe vecs...\")\n",
    "        X_full, X_middle = self._extract_training_embeddings(X_full_embs, X_middle_embs, train_flag, concat)\n",
    "        X = np.concatenate((X_full, X_middle), axis = 1)\n",
    "        np.save(self.save_paths[\"features\"] + train_flag, X)\n",
    "        print(\"Done!\")\n",
    "        return Y, X_full, X_middle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sanity_check = train_data[:20000]\n",
    "server = StanfordCoreNLP('http://localhost:9000')\n",
    "embedding_path = \"./glove/glove.6B.50d.txt\"\n",
    "\n",
    "fe_args = {\n",
    "    \"server\" : server, \n",
    "    \"embedding_path\" : embedding_path\n",
    "}\n",
    "\n",
    "save_paths = {\n",
    "    \"train_full_save_path\" : \"train_features_full/\", \n",
    "    \"train_middle_save_path\" : \"train_features_middle/\",\n",
    "    \"test_full_save_path\" : \"test_features_full/\",\n",
    "    \"test_middle_save_path\" : \"test_features_middle/\",\n",
    "    \"train_full_g2v_path\" : \"train_features_full_g2v/features.csv\",\n",
    "    \"train_middle_g2v_path\" : \"train_features_middle_g2v/features.csv\",\n",
    "    \"test_full_g2v_path\" : \"test_features_full_g2v/features.csv\", \n",
    "    \"test_middle_g2v_path\" : \"test_features_middle_g2v/features.csv\",\n",
    "    \"responses\" : \"all_features/responses\",\n",
    "    \"features\" : \"all_features/features\"\n",
    "}\n",
    "\n",
    "n_samples = len(sanity_check)\n",
    "print(\"Originally {} samples in training data...\".format(n_samples))\n",
    "fep = FeatureExtractorPipeline(fe_args, save_paths)\n",
    "Y, X_full, X_middle = fep.extract_features(sanity_check, total = 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's just verify really quickly that:\n",
    "1. The one-hot encoding of the labels is correct. \n",
    "2. Resampling _did_ fix any class-imbalance issues (if there were any)..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The Alcatel-Lucent company that will operate this business will be known as Thompson Advisory Group LLC .\n",
      "Label: org:subsidiaries\n",
      "FEP label: no_relation\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-ce24929ab479>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mfep_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclass_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FEP label: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfep_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mfep_label\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"# classes: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Verifying that the labels are correct\n",
    "idx = random.choice(range(len(sanity_check)))\n",
    "ex = \" \".join(sanity_check[idx][\"token\"])\n",
    "label = sanity_check[idx][\"relation\"]\n",
    "print(\"Sentence: {}\\nLabel: {}\".format(ex, label))\n",
    "\n",
    "class_labels = { v : k for k, v in fep.get_class_labels().items()}\n",
    "fep_label = class_labels[np.argmax(Y[idx]).astype(int)]\n",
    "print(\"FEP label: {}\".format(fep_label))\n",
    "assert fep_label == label\n",
    "\n",
    "print(\"# classes: {}\".format(len(class_labels)))\n",
    "print(class_labels)\n",
    "# Verifying class-imbalance issues\n",
    "labels = np.argmax(Y, axis = 1)\n",
    "keys, counts = np.unique(labels, return_counts = True)\n",
    "plt.bar(keys, counts)\n",
    "plt.title(\"Class distribution in resampled data\")\n",
    "plt.xticks(keys, [class_labels[k] for k in keys], rotation = 270)\n",
    "plt.show()\n",
    "\n",
    "# Verifying that all the shapes work out...\n",
    "print(np.array(Y).shape)\n",
    "print(X_full.shape)\n",
    "print(X_middle.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "\n",
    "class FullyConnectedNN(nn.Module):\n",
    "    def __init__(self, architecture, input_size, output_size):\n",
    "        super().__init__()\n",
    "        architecture = [input_size] + architecture\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i, neurons in enumerate(architecture[:-1]):\n",
    "            self.layers.append(nn.Linear(architecture[i], architecture[i+1]))\n",
    "        self.output_layer = nn.Linear(architecture[-1], output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            x = F.relu(layer(x))\n",
    "        return self.output_layer(x).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's sanity-check just to make sure the network's forward pass is functional..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "net = FullyConnectedNN([5, 5, 5], 10, 2)\n",
    "x = torch.rand(10).view(1, -1)\n",
    "y = net(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we write the training loop..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "class RelExtDataset(Dataset):\n",
    "    def __init__(self, X_path, Y_path, device):\n",
    "        self.X = torch.from_numpy(np.load(X_path)).float().squeeze().to(device)\n",
    "        self.Y = torch.from_numpy(np.load(Y_path)).float().squeeze().to(device)\n",
    "        assert len(self.X) == len(self.Y)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.Y[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.Y)\n",
    "    \n",
    "class Trainer(object):\n",
    "    def __init__(self, model, hyperparams, device):\n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.hyperparams = hyperparams\n",
    "        self.lossfn = self.hyperparams[\"loss_function\"]\n",
    "        self.optimizer = self.hyperparams[\"optimizer\"](self.model.parameters(), lr = self.hyperparams[\"lr\"])\n",
    "        self.scheduler = self.hyperparams.get(\"scheduler\", None)\n",
    "        if self.scheduler:\n",
    "            self.scheduler = torch.optim.lr_scheduler.LambdaLR(self.optimizer, lr_lambda = self.scheduler)\n",
    "            del self.hyperparams[\"scheduler\"]\n",
    "        \n",
    "        self.dataset = None\n",
    "        self.epochs_trained = 0\n",
    "    \n",
    "    def load_data(self, X_path, Y_path):\n",
    "        dataset = RelExtDataset(X_path, Y_path, self.device)\n",
    "        self.dataset = DataLoader(dataset, batch_size = self.hyperparams[\"batch_size\"], num_workers = 0)\n",
    "        \n",
    "    def train(self, n_epochs, log_path, checkpoint_path, print_every = 10):\n",
    "        assert self.dataset\n",
    "        losses = []\n",
    "        \n",
    "        if log_path:\n",
    "            writer = SummaryWriter(log_path)\n",
    "            \n",
    "        n_batches = 0\n",
    "        for epoch in range(n_epochs):\n",
    "            for batch_i, (X, Y) in enumerate(self.dataset):\n",
    "                self.optimizer.zero_grad()\n",
    "                Yhat = self.model(X)\n",
    "                \n",
    "                loss = self.lossfn(Yhat, Y)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                n_batches += 1\n",
    "                if (n_batches % print_every == 0):\n",
    "                    if log_path:\n",
    "                        writer.add_scalar(\"loss\", loss, n_batches)\n",
    "                    \n",
    "                    loss_p = loss.detach().numpy()\n",
    "                    losses.append(loss_p)\n",
    "                    print(\"Epoch {0}, batch {1}: training loss = {2:.4f}\".format(epoch+1, n_batches, loss_p), end = \"\\r\")\n",
    "                    sys.stdout.flush()\n",
    "                \n",
    "            self.epochs_trained += 1\n",
    "            if self.scheduler:\n",
    "                self.scheduler.step()\n",
    "            \n",
    "            print(\"\")\n",
    "            \n",
    "            if checkpoint_path:\n",
    "                with open(checkpoint_path + \"checkpoint_epoch_{}.pkl\".format(epoch+1), \"wb\") as f:\n",
    "                    pickle.dump(self.get_pickleable_model(), f)\n",
    "                    \n",
    "        if log_path:\n",
    "            writer.close()\n",
    "            \n",
    "        return losses\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "    \n",
    "    def get_pickleable_model(self):\n",
    "        pkl = {\n",
    "            \"hyperparams\" : self.hyperparams, \n",
    "            \"epochs_trained\" : self.epochs_trained, \n",
    "            \"model_params\" : self.model.state_dict(), \n",
    "            \"optimizer_params\" : self.optimizer.state_dict()\n",
    "        }\n",
    "        return pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Training the model (!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 356\n",
    "output_size = 42\n",
    "architecture = [128, 64, 64, 64, 64]\n",
    "model = FullyConnectedNN(architecture, input_size, output_size)\n",
    "hyperparams = {\n",
    "    \"lr\" : 2.5e-6, \n",
    "    \"batch_size\" : 64, \n",
    "    \"optimizer\" : torch.optim.Adam, \n",
    "    \"scheduler\" : lambda ep : 0.9**ep, \n",
    "    \"loss_function\" : torch.nn.BCEWithLogitsLoss()\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "trainer = Trainer(model, hyperparams, device)\n",
    "\n",
    "X_path = save_paths[\"features\"] + \"train.npy\"\n",
    "Y_path = save_paths[\"responses\"] + \"train.npy\"\n",
    "trainer.load_data(X_path, Y_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = \"logs/model_1/\"\n",
    "checkpoint_path = \"checkpoints/model_1/\"\n",
    "n_epochs = 50\n",
    "losses = trainer.train(n_epochs, log_path, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can evaluate the model. To do that, we first need to extract the features and responses for the test dataset, which we can do using the same FEP... \n",
    "\n",
    "We then evaluate by printing a classification report (focusing on macro-averaged F1 score) as well as a confusion matrix to visualize over which relations the graph features perform particularly poorly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test, X_full_test, X_middle_test = fep.extract_features(test_data, total = 0, train_flag = \"test\")\n",
    "X_test = torch.from_numpy(np.concatenate((X_full_test, X_middle_test), axis = 1)).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as m\n",
    "\n",
    "model = trainer.get_model()\n",
    "model.eval()\n",
    "Yhat_test = model(X_test).detach().numpy()\n",
    "\n",
    "n, d = Yhat_test.shape\n",
    "assert d == output_size\n",
    "print(Yhat_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_labels = np.argmax(Y_test, axis = 1)\n",
    "pred_labels = np.argmax(Yhat_test, axis = 1)\n",
    "\n",
    "class_report = m.classification_report(gt_labels, pred_labels)\n",
    "confusion = m.confusion_matrix(gt_labels, pred_labels)\n",
    "\n",
    "print(class_report)\n",
    "print(confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
